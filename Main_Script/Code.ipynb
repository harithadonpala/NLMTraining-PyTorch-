{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4JCjYsecTx5"
      },
      "source": [
        "Assignment-2(Neural Language Model Training (PyTorch))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWZe2Rjccll5"
      },
      "source": [
        "Step-1:Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FNPmqjQceHW",
        "outputId": "4b95ba5e-1bb1-4ef4-c9b5-53e4ed0af948"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 500 characters of the dataset:\n",
            "The Project Gutenberg eBook, Pride and Prejudice, by Jane Austen, Edited\n",
            "by R. W. (Robert William) Chapman\n",
            "\n",
            "\n",
            "This eBook is for the use of anyone anywhere at no cost and with\n",
            "almost no restrictions whatsoever.  You may copy it, give it away or\n",
            "re-use it under the terms of the Project Gutenberg License included\n",
            "with this eBook or online at www.gutenberg.org\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Title: Pride and Prejudice\n",
            "\n",
            "\n",
            "Author: Jane Austen\n",
            "\n",
            "Editor: R. W. (Robert William) Chapman\n",
            "\n",
            "Release Date: May 9, 2013  [eBook #42671]\n",
            "\n",
            "Lang\n",
            "\n",
            "Total number of characters in the dataset: 711331\n"
          ]
        }
      ],
      "source": [
        "file_path = 'Pride_and_Prejudice-Jane_Austen.txt'\n",
        "\n",
        "try:\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text_data = file.read()\n",
        "\n",
        "    print(\"First 500 characters of the dataset:\")    # for reference printing the first 500 characters and length of the dataset\n",
        "    print(text_data[:500])\n",
        "    print(f\"\\nTotal number of characters in the dataset: {len(text_data)}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{file_path}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMMZiNQNdHC1"
      },
      "source": [
        "Step-2: Data Preprocessing - Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-ICoBdXc3vB",
        "outputId": "524b6a6c-b951-4773-eb57-7d73dc6d3872"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 20 tokens:\n",
            "['the', 'project', 'gutenberg', 'ebook', 'pride', 'and', 'prejudice', 'by', 'jane', 'austen', 'edited', 'by', 'r', 'w', 'robert', 'william', 'chapman', 'this', 'ebook', 'is']\n",
            "\n",
            "Total number of tokens: 126711\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Convert text_data to lowercase\n",
        "text_data_lower = text_data.lower()\n",
        "\n",
        "# Use re.findall to extract words (tokens)\n",
        "tokens = re.findall(r'\\b\\w+\\b', text_data_lower)\n",
        "\n",
        "# Print the first 20 tokens and the total number of tokens\n",
        "print(\"First 20 tokens:\")\n",
        "print(tokens[:20])\n",
        "print(f\"\\nTotal number of tokens: {len(tokens)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTCJaQSOdPJK"
      },
      "source": [
        "Step-3: Data Preprocessing - Vocabulary Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYMEy2ZRcSjs",
        "outputId": "5a9c50ff-eb74-4fce-9463-00934b3c7f1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total vocabulary size: 6763\n",
            "\n",
            "Sample word_to_idx mappings:\n",
            "'the': 6036\n",
            "'and': 453\n",
            "'prejudice': 4621\n",
            "'<unk>': 0\n",
            "'elizabeth': 2144\n",
            "'darcy': 1596\n",
            "\n",
            "Sample idx_to_word mappings:\n",
            "0: '<unk>' \n",
            "1: '000' \n",
            "2: '1' \n",
            "3: '1500' \n",
            "6762: 'Ã ' \n",
            "6761: 'zip' \n"
          ]
        }
      ],
      "source": [
        "unique_tokens = sorted(list(set(tokens)))\n",
        "\n",
        "# Define the unknown token\n",
        "UNK_TOKEN = '<unk>'\n",
        "\n",
        "# Initialize dictionaries\n",
        "word_to_idx = {UNK_TOKEN: 0}\n",
        "idx_to_word = {0: UNK_TOKEN}\n",
        "\n",
        "# Assign IDs to unique tokens, starting from 1\n",
        "for i, word in enumerate(unique_tokens):\n",
        "    word_to_idx[word] = i + 1\n",
        "    idx_to_word[i + 1] = word\n",
        "\n",
        "# Total vocabulary size\n",
        "vocabulary_size = len(word_to_idx)\n",
        "\n",
        "print(f\"Total vocabulary size: {vocabulary_size}\")\n",
        "\n",
        "print(\"\\nSample word_to_idx mappings:\")\n",
        "sample_words = ['the', 'and', 'prejudice', UNK_TOKEN, 'elizabeth', 'darcy']\n",
        "for word in sample_words:\n",
        "    if word in word_to_idx:\n",
        "        print(f\"'{word}': {word_to_idx[word]}\")\n",
        "    else:\n",
        "        print(f\"'{word}': (Not in vocab, maps to UNK: {word_to_idx[UNK_TOKEN]}) \")\n",
        "\n",
        "print(\"\\nSample idx_to_word mappings:\")\n",
        "sample_ids = [0, 1, 2, 3, vocabulary_size - 1, vocabulary_size - 2]\n",
        "for idx in sample_ids:\n",
        "    if idx in idx_to_word:\n",
        "        print(f\"{idx}: '{idx_to_word[idx]}' \")\n",
        "    else:\n",
        "        print(f\"{idx}: (ID not found) \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HvW2-dHdYOG"
      },
      "source": [
        "Data Preprocessing - Dataset and DataLoader Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a35u4AlSdXmC",
        "outputId": "60eac050-df4c-4a22-b423-845a03aa3904"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 20 numerical tokens: [6036, 4729, 2898, 2093, 4674, 453, 4621, 965, 3482, 672, 2104, 965, 4857, 6510, 5237, 6637, 1068, 6062, 2093, 3473]\n",
            "Total numerical tokens: 126711\n",
            "\n",
            "Defined sequence length: 50\n",
            "\n",
            "Train data size: 101368\n",
            "Validation data size: 12671\n",
            "Test data size: 12672\n",
            "\n",
            "Train dataset (number of sequences): 101318\n",
            "Validation dataset (number of sequences): 12621\n",
            "Test dataset (number of sequences): 12622\n",
            "\n",
            "Batch size: 64\n",
            "Number of training batches: 1583\n",
            "Number of validation batches: 197\n",
            "Number of test batches: 197\n",
            "\n",
            "Sample batch shapes - Inputs: torch.Size([64, 50]), Targets: torch.Size([64, 50])\n",
            "Sample input sequence (first in batch): [453, 6039, 2726, 5874, 6102, 6036, 2668, 4184, 6039, 4764, 960, 2144, 5313, 6662, 300, 4184, 3035, 5978, 6035, 3474, 6543, 4055, 2751, 4098, 6400, 2556, 6662, 3634, 4184, 5692, 453, 3966, 4903, 2134, 6027, 6036, 2726, 4184, 5248, 453, 4184, 6062, 4495, 6068, 5470, 3111, 3883, 2965, 750, 3937]\n",
            "Sample target sequence (first in batch): [6039, 2726, 5874, 6102, 6036, 2668, 4184, 6039, 4764, 960, 2144, 5313, 6662, 300, 4184, 3035, 5978, 6035, 3474, 6543, 4055, 2751, 4098, 6400, 2556, 6662, 3634, 4184, 5692, 453, 3966, 4903, 2134, 6027, 6036, 2726, 4184, 5248, 453, 4184, 6062, 4495, 6068, 5470, 3111, 3883, 2965, 750, 3937, 6662]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# 1. Convert tokens to numerical IDs\n",
        "numerical_tokens = []\n",
        "for token in tokens:\n",
        "    numerical_tokens.append(word_to_idx.get(token, word_to_idx[UNK_TOKEN]))\n",
        "\n",
        "numerical_tokens = torch.tensor(numerical_tokens, dtype=torch.long)\n",
        "\n",
        "print(f\"First 20 numerical tokens: {numerical_tokens[:20].tolist()}\")\n",
        "print(f\"Total numerical tokens: {len(numerical_tokens)}\")\n",
        "\n",
        "# 2. Define sequence length\n",
        "sequence_length = 50\n",
        "print(f\"\\nDefined sequence length: {sequence_length}\")\n",
        "\n",
        "# 3. Create a custom PyTorch Dataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, sequence_length):\n",
        "        self.data = data\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "    def __len__(self):\n",
        "        # Number of possible sequences. Each sequence of length `sequence_length`\n",
        "        # has a target sequence of the same length, shifted by one.\n",
        "        # E.g., for data [t1, t2, t3, t4, t5] and seq_len 3:\n",
        "        # Input: [t1, t2, t3], Target: [t2, t3, t4]\n",
        "        # Input: [t2, t3, t4], Target: [t3, t4, t5]\n",
        "        # There are (len(data) - sequence_length) such input-target pairs.\n",
        "        return len(self.data) - self.sequence_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_sequence = self.data[idx : idx + self.sequence_length]\n",
        "        target_sequence = self.data[idx + 1 : idx + self.sequence_length + 1]\n",
        "        return input_sequence, target_sequence\n",
        "\n",
        "# 4. Split the numerical tokens into training, validation, and test sets\n",
        "# We'll use 80% for training, 10% for validation, and 10% for testing\n",
        "total_size = len(numerical_tokens)\n",
        "train_size = int(0.8 * total_size)\n",
        "val_size = int(0.1 * total_size)\n",
        "test_size = total_size - train_size - val_size # Ensure all tokens are covered\n",
        "\n",
        "train_data = numerical_tokens[:train_size]\n",
        "val_data = numerical_tokens[train_size : train_size + val_size]\n",
        "test_data = numerical_tokens[train_size + val_size :]\n",
        "\n",
        "print(f\"\\nTrain data size: {len(train_data)}\")\n",
        "print(f\"Validation data size: {len(val_data)}\")\n",
        "print(f\"Test data size: {len(test_data)}\")\n",
        "\n",
        "# 5. Create instances of TextDataset for each split\n",
        "train_dataset = TextDataset(train_data, sequence_length)\n",
        "val_dataset = TextDataset(val_data, sequence_length)\n",
        "test_dataset = TextDataset(test_data, sequence_length)\n",
        "\n",
        "print(f\"\\nTrain dataset (number of sequences): {len(train_dataset)}\")\n",
        "print(f\"Validation dataset (number of sequences): {len(val_dataset)}\")\n",
        "print(f\"Test dataset (number of sequences): {len(test_dataset)}\")\n",
        "\n",
        "# 6. Initialize DataLoaders\n",
        "batch_size = 64\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "\n",
        "print(f\"\\nBatch size: {batch_size}\")\n",
        "print(f\"Number of training batches: {len(train_dataloader)}\")\n",
        "print(f\"Number of validation batches: {len(val_dataloader)}\")\n",
        "print(f\"Number of test batches: {len(test_dataloader)}\")\n",
        "\n",
        "# Verify a single batch\n",
        "for inputs, targets in train_dataloader:\n",
        "    print(f\"\\nSample batch shapes - Inputs: {inputs.shape}, Targets: {targets.shape}\")\n",
        "    print(f\"Sample input sequence (first in batch): {inputs[0].tolist()}\")\n",
        "    print(f\"Sample target sequence (first in batch): {targets[0].tolist()}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J00WpM5Bdvnh"
      },
      "source": [
        "Step-4 : Implement a neural language model using PyTorch, such as an LSTM, GRU, or simple RNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cPhAzGidx2L",
        "outputId": "8a93ae3b-1b72-4c92-ba51-044c212528db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LanguageModel(\n",
            "  (embedding): Embedding(6763, 128)\n",
            "  (lstm): LSTM(128, 256, num_layers=2, batch_first=True, dropout=0.3)\n",
            "  (linear): Linear(in_features=256, out_features=6763, bias=True)\n",
            ")\n",
            "\n",
            "Model instantiated with:\n",
            "  Embedding Dim: 128\n",
            "  Hidden Size: 256\n",
            "  Num Layers: 2\n",
            "  Dropout Rate: 0.3\n",
            "  Vocabulary Size: 6763\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Define hyperparameters for the model\n",
        "embedding_dim = 128\n",
        "hidden_size = 256\n",
        "num_layers = 2\n",
        "dropout_rate = 0.3 # Dropout for the LSTM layers\n",
        "\n",
        "# 2. Define a Python class for your neural language model\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, vocabulary_size, embedding_dim, hidden_size, num_layers, dropout_rate=0.0):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # 3. Define the layers\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocabulary_size, embedding_dim)\n",
        "\n",
        "        # Recurrent layer (LSTM)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout_rate, # Dropout applied to the output of each LSTM layer except the last\n",
        "            batch_first=True # Input and output tensors are provided as (batch, seq, feature)\n",
        "        )\n",
        "\n",
        "        # Linear output layer\n",
        "        self.linear = nn.Linear(hidden_size, vocabulary_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 4. Implement the forward pass\n",
        "        # Pass input through embedding layer\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        # Pass embedded input through LSTM layer\n",
        "        # lstm_output shape: (batch_size, sequence_length, hidden_size)\n",
        "        # _ contains (h_n, c_n) - hidden state and cell state of the last layer\n",
        "        lstm_output, _ = self.lstm(embedded)\n",
        "\n",
        "        # Reshape the output to apply the linear layer to each time step's output\n",
        "        # From (batch_size, sequence_length, hidden_size) to (batch_size * sequence_length, hidden_size)\n",
        "        reshaped_output = lstm_output.reshape(-1, self.hidden_size)\n",
        "\n",
        "        # Pass reshaped output through linear layer\n",
        "        output = self.linear(reshaped_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "# 5. Instantiate the model\n",
        "# vocabulary_size was determined in previous steps\n",
        "model = LanguageModel(vocabulary_size, embedding_dim, hidden_size, num_layers, dropout_rate)\n",
        "\n",
        "# 6. Print the model architecture\n",
        "print(model)\n",
        "\n",
        "print(f\"\\nModel instantiated with:\\n  Embedding Dim: {embedding_dim}\\n  Hidden Size: {hidden_size}\\n  Num Layers: {num_layers}\\n  Dropout Rate: {dropout_rate}\\n  Vocabulary Size: {vocabulary_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWNgAHwyeFXJ"
      },
      "source": [
        "Step-5 : Defining the loss function (e.g., nn.CrossEntropyLoss) and an optimizer (e.g., Adam, SGD) for training the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnOIzsC8eNJC",
        "outputId": "631be79f-b9a2-4cf2-814d-501bdde28bf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss function: CrossEntropyLoss()\n",
            "Optimizer: Adam with learning rate: 0.001\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 1. Define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 2. Define the optimizer\n",
        "learning_rate = 0.001\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "print(f\"Loss function: {criterion}\")\n",
        "print(f\"Optimizer: {optimizer.__class__.__name__} with learning rate: {learning_rate}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9M34fA1eUzj"
      },
      "source": [
        "Step-6: Developing the core training loop, including forward pass, backpropagation, gradient clipping (if necessary), and optimizer step for a given number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OW8ryNU4eUer",
        "outputId": "8a0f09cf-c369-44b9-e9f1-14ba8905a5ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Number of epochs: 10\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# 1. Set up device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if device.type == 'cpu':\n",
        "    print(\"\\n*** IMPORTANT: For faster training, consider enabling GPU acceleration! ***\")\n",
        "    print(\"To do this, go to 'Runtime' -> 'Change runtime type' in the Colab menu, then select 'T4 GPU' (or similar) as the hardware accelerator.\")\n",
        "\n",
        "# 2. Define number of training epochs\n",
        "num_epochs = 10\n",
        "print(f\"Number of epochs: {num_epochs}\")\n",
        "\n",
        "# 3. Initialize lists to store training and validation loss\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# 4. Start the main training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # 5. Training phase\n",
        "    model.train() # Set model to training mode\n",
        "    total_train_loss = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n",
        "        # d. Move inputs and targets to device\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # e. Zero out the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # f. Perform a forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # g. Reshape targets for CrossEntropyLoss. The outputs are (batch_size * sequence_length, vocabulary_size)\n",
        "        # so targets should be (batch_size * sequence_length)\n",
        "        loss = criterion(outputs, targets.reshape(-1))\n",
        "\n",
        "        # i. Perform a backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # j. Apply gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # k. Update model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # l. Accumulate loss\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    # m. Calculate average training loss for the epoch\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # 6. Validation phase\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad(): # Disable gradient calculations\n",
        "        for inputs, targets in val_dataloader:\n",
        "            # e. Move inputs and targets to device\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # f. Perform a forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # g. Reshape targets for CrossEntropyLoss\n",
        "            loss = criterion(outputs, targets.reshape(-1))\n",
        "\n",
        "            # i. Accumulate loss\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    # j. Calculate average validation loss for the epoch\n",
        "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "print(\"\\nTraining complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step -7 : Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# -----------------------------------------\n",
        "# Helper function to plot loss curves\n",
        "# -----------------------------------------\n",
        "def plot_loss(train, val, title):\n",
        "    plt.figure(figsize=(7,5))\n",
        "    plt.plot(train, label='Training Loss', linewidth=2)\n",
        "    plt.plot(val, label='Validation Loss', linewidth=2)\n",
        "    plt.xlabel(\"Epochs\", fontsize=12)\n",
        "    plt.ylabel(\"Loss\", fontsize=12)\n",
        "    plt.title(title, fontsize=15)\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# -----------------------------------------\n",
        "# 1. UNDERFITTING\n",
        "# Both losses are high, model hasn't learned enough\n",
        "# -----------------------------------------\n",
        "underfit_train = np.linspace(1.4, 1.15, 12)   # high loss, slight improvement\n",
        "underfit_val   = np.linspace(1.45, 1.25, 12)\n",
        "\n",
        "plot_loss(underfit_train, underfit_val, \"Underfitting: Model Undertrained / Too Simple\")\n",
        "\n",
        "# -----------------------------------------\n",
        "# 2. OVERFITTING\n",
        "# Training loss keeps dropping, validation loss rises\n",
        "# -----------------------------------------\n",
        "overfit_train = np.linspace(0.9, 0.18, 20)\n",
        "overfit_val   = [0.95,0.85,0.75,0.70,0.68,0.69,0.71,0.75,0.80,0.88,\n",
        "                 1.0,1.15,1.30,1.45,1.60,1.75,1.90,2.05,2.15,2.25]\n",
        "\n",
        "plot_loss(overfit_train, overfit_val, \"Overfitting: Model Memorizes Training Data\")\n",
        "\n",
        "# -----------------------------------------\n",
        "# 3. BEST FIT (GOOD GENERALIZATION)\n",
        "# Validation follows training curve closely\n",
        "# -----------------------------------------\n",
        "best_train = np.linspace(0.9, 0.22, 15)\n",
        "best_val   = best_train + np.random.uniform(-0.03, 0.03, 15)  # small noise\n",
        "\n",
        "plot_loss(best_train, best_val, \"Best Fit: Good Generalization\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "step -7 : Visualization - Loss Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "\n",
        "plt.plot(train_losses, label=\"Training Loss\", linewidth=2)\n",
        "plt.plot(val_losses, label=\"Validation Loss\", linewidth=2)\n",
        "\n",
        "plt.title(\"Training vs Validation Loss\", fontsize=16)\n",
        "plt.xlabel(\"Epoch\", fontsize=13)\n",
        "plt.ylabel(\"Loss\", fontsize=13)\n",
        "plt.grid(True)\n",
        "plt.legend(fontsize=12)\n",
        "\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
